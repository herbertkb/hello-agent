== Hello Agent (Camel Standalone)

This is a playground project for experimenting with the Camel https://camel.apache.org/components/next/langchain4j-agent-component.html[langchain4j-agent] component.

It uses the latest version of Camel (4.17) to try out the latest features before they are available in 
the runtime versions.  

The experiments are written `CamelTestSupport` unit test classes.
They are `@Disabled` by default so they don't hammer your LLM if you do a `mvn package`.
I should port them to JBang before sharing them more broadly. 

These routes assume a LLM is running locally.
You can run the `granite4:1b` model locally with an Ollama container with

[source,sh]
----
podman run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
podman exec -it ollama ollama run granite4:1b
----

Or see the https://docs.ollama.com/[Ollama docs].

I also installed and ran it on Fedora with 

[source,sh]
----
sudo dnf install ollama
ollama run granite4:1b
----

I use `granite4:1b` because `granite4-tiny-h` freezes my laptop for 15 minutes on every request.
`granite4:350m` runs quickly but inferences poorly.
`granite4:1b` takes a few seconds per query but does a better job. 

You can use the OpenAI model by adding its dependency
[source,xml]
----
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-ollama</artifactId>
    <version>1.9.1</version>
</dependency> 
----

And replacing the `ChatModel` class with
[source,java]
----
// demo model
//https://docs.langchain4j.dev/integrations/language-models/open-ai
OpenAiChatModel openAiModel = OpenAiChatModel.builder()
        .baseUrl("http://langchain4j.dev/demo/openai/v1")
        .apiKey("demo")
        .modelName("gpt-4o-mini")
        .build();
----